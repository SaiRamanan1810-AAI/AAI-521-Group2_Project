{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e01ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports and path\n",
    "import os, sys, shutil\n",
    "from pathlib import Path\n",
    "print('cwd:', os.getcwd())\n",
    "# ensure project root is importable\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else '.', '..')))\n",
    "# Import libs we need\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import random\n",
    "print('python', sys.version.split()[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)\n",
    "print('PIL', Image.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327635fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny synthetic dataset (images are plain RGB squares)\n",
    "root = Path('data_synthetic')\n",
    "if root.exists():\n",
    "    shutil.rmtree(root)\n",
    "(root / 'plants').mkdir(parents=True, exist_ok=True)\n",
    "(root / 'diseases').mkdir(parents=True, exist_ok=True)\n",
    "# Create 4 species with imbalanced counts\n",
    "species = {'Cashew': 10, 'Cassava': 4, 'Maize': 2, 'Tomato': 8}  # imbalanced counts\n",
    "for sp, count in species.items():\n",
    "    d = root / 'plants' / sp\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(count):\n",
    "        img = Image.new('RGB', (300,300), (random.randint(0,255), random.randint(0,255), random.randint(0,255)))\n",
    "        img.save(d / f'{sp.lower()}_{i}.jpg')\n",
    "# Create diseases for Cashew with two disease classes, imbalanced\n",
    "droot = root / 'diseases' / 'Cashew'\n",
    "(droot / 'LeafSpot').mkdir(parents=True, exist_ok=True)\n",
    "(droot / 'Healthy').mkdir(parents=True, exist_ok=True)\n",
    "# LeafSpot few samples, Healthy many\n",
    "for i in range(3):\n",
    "    img = Image.new('RGB', (400,400), (255,0,0))\n",
    "    img.save(droot / 'LeafSpot' / f'leaf_{i}.jpg')\n",
    "for i in range(12):\n",
    "    img = Image.new('RGB', (400,400), (0,255,0))\n",
    "    img.save(droot / 'Healthy' / f'healthy_{i}.jpg')\n",
    "print('Created synthetic dataset at', root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use src.data utilities to prepare splits and inspect counts\n",
    "from src.data import prepare_plant_dataset, prepare_disease_dataset, list_images, make_weighted_sampler\n",
    "plants = prepare_plant_dataset(str(root / 'plants'))\n",
    "print('Species meta:', plants['meta'])\n",
    "for split in ['train','val','test']:\n",
    "    labs = [lab for _, lab in plants[split]]\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    print(f'{split} counts ->', dict(zip(unique.tolist(), counts.tolist())))\n",
    "# disease dataset for Cashew\n",
    "cashew = prepare_disease_dataset('Cashew', data_dir=str(root / 'diseases'))\n",
    "print('Cashew disease classes:', cashew['meta']['classes'])\n",
    "for split in ['train','val','test']:\n",
    "    labs = [lab for _, lab in cashew[split]]\n",
    "    unique, counts = np.unique(labs, return_counts=True)\n",
    "    print(f'Cashew {split} counts ->', dict(zip(unique.tolist(), counts.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8afa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate WeightedRandomSampler oversampling distribution\n",
    "from src.data import SimpleImageDataset, get_transforms\n",
    "train_samples = plants['train']\n",
    "sampler, class_weights = make_weighted_sampler(train_samples)\n",
    "print('class_weights (for loss):', class_weights)\n",
    "# Create dataset with heavier transforms for minority classes\n",
    "tf_default = get_transforms('plant')\n",
    "# detect minority classes: simple threshold median rule\n",
    "labels = [lab for _, lab in train_samples]\n",
    "uniques, counts = np.unique(labels, return_counts=True)\n",
    "median = np.median(counts)\n",
    "transform_map = {}\n",
    "for cls, cnt in zip(uniques, counts):\n",
    "    if cnt < median:\n",
    "        # use disease-style heavy augment for minority plant classes\n",
    "        transform_map[int(cls)] = get_transforms('disease')\n",
    "ds = SimpleImageDataset(train_samples, transform=tf_default, transform_map=transform_map)\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(ds, batch_size=1, sampler=sampler)\n",
    "# sample 1000 times and count labels\n",
    "counts_sampled = {}\n",
    "it = iter(loader)\n",
    "for i in range(200):\n",
    "    try:\n",
    "        xb, yb, p = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(loader)\n",
    "        xb, yb, p = next(it)\n",
    "    lab = int(yb.item())\n",
    "    counts_sampled[lab] = counts_sampled.get(lab, 0) + 1\n",
    "print('Sampled label distribution (200 draws):', counts_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small forward-pass smoke test with EfficientNet-B0\n",
    "from src.model import load_efficientnet_b0\n",
    "model = load_efficientnet_b0(num_classes=4, pretrained=False)\n",
    "model.eval()\n",
    "# get one batch from loader\n",
    "it = iter(loader)\n",
    "xb, yb, paths = next(it)\n",
    "with torch.no_grad():\n",
    "    out = model(xb)\n",
    "print('out shape:', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a 1-epoch head + 1-epoch finetune two-step training smoke test (very small)\n",
    "from src.train import train_two_step\n",
    "# build tiny dataloaders dict using the sampler and small batch\n",
    "val_ds = SimpleImageDataset(plants['val'], transform=tf_default)\n",
    "test_ds = SimpleImageDataset(plants['test'], transform=tf_default)\n",
    "dls = {\n",
    "    'train': DataLoader(ds, batch_size=4, sampler=sampler),\n",
    "    'val': DataLoader(val_ds, batch_size=4, shuffle=False),\n",
    "    'test': DataLoader(test_ds, batch_size=4, shuffle=False),\n",
    "}\n",
    "device = torch.device('cpu')\n",
    "model_small = load_efficientnet_b0(num_classes=4, pretrained=False)\n",
    "model_trained, history = train_two_step(model_small, dls, device, epochs_head=1, epochs_finetune=1, lr_head=1e-3, lr_ft=1e-4, class_weights=class_weights, mixup_alpha=0.2)\n",
    "print('history keys:', history.keys())\n",
    "print('history val_f1:', history.get('val_f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e11393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick run using focal loss to ensure code path runs (1 epoch each)\n",
    "model_small2 = load_efficientnet_b0(num_classes=4, pretrained=False)\n",
    "model_trained2, history2 = train_two_step(model_small2, dls, device, epochs_head=1, epochs_finetune=1, class_weights=class_weights, use_focal=True, focal_gamma=2.0, mixup_alpha=0.2)\n",
    "print('focal history val_f1:', history2.get('val_f1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761f655",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- This notebook created a small synthetic dataset, demonstrated that `make_weighted_sampler` oversamples minority classes, applied class-specific heavy augmentations for minority labels, and ran a 1-epoch two-step training smoke test with MixUp and focal-loss code paths.\n",
    "- Next steps: run the same notebook on your real `data/` tree (adjust paths) and increase epochs, enable GPU, and tune hyperparameters (mixup alpha, focal gamma) as needed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
